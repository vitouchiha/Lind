import asyncio
import logging
import re
import base64
import json
import os
import gzip
import zlib
import zstandard
import random
import time
from urllib.parse import urlparse, quote_plus
import aiohttp
from aiohttp import ClientSession, ClientTimeout, TCPConnector, FormData
from aiohttp_socks import ProxyConnector
from typing import Dict, Any, Optional
from urllib.parse import urljoin

logger = logging.getLogger(__name__)

class ExtractorError(Exception):
    pass

class DLHDExtractor:
    """DLHD Extractor con sessione persistente e gestione anti-bot avanzata"""


    def __init__(self, request_headers: dict, proxies: list = None):
        self.request_headers = request_headers
        self.base_headers = {
            # ‚úÖ User-Agent pi√π recente per bypassare protezioni anti-bot
            "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36",
        }
        self.session = None
        self.mediaflow_endpoint = "hls_manifest_proxy"
        self._session_lock = asyncio.Lock()
        self.proxies = proxies or []
        self._extraction_locks: Dict[str, asyncio.Lock] = {} # ‚úÖ NUOVO: Lock per evitare estrazioni multiple
        self.cache_file = os.path.join(os.path.dirname(__file__), '.dlhd_cache')
        
        # Carica cache e inizializza host
        cache_data = self._load_cache()
        self._stream_data_cache: Dict[str, Dict[str, Any]] = cache_data.get('streams', {})
        
        # ‚úÖ Lista host iframe (caricata da cache o vuota)
        self.iframe_hosts = cache_data.get('hosts', [])
        
        # ‚úÖ Configurazione server dinamica dal worker (usando TEMPLATE completi)
        # Tutti i valori provengono dal worker, i fallback sono solo per il primo avvio
        self.auth_url = cache_data.get('auth_url', 'https://security.kiko2.ru/auth2.php')
        self.stream_cdn_template = cache_data.get('stream_cdn_template', 'https://top1.kiko2.ru/top1/cdn/{CHANNEL}/mono.css')
        self.stream_other_template = cache_data.get('stream_other_template', 'https://{SERVER_KEY}new.kiko2.ru/{SERVER_KEY}/{CHANNEL}/mono.css')
        self.heartbeat_url = cache_data.get('heartbeat_url', 'https://chevy.kiko2.ru/heartbeat')
        self.server_lookup_url = cache_data.get('server_lookup_url', 'https://chevy.kiko2.ru/server_lookup')
        self.base_domain = cache_data.get('base_domain', 'kiko2.ru')
        
        logger.info(f"Hosts caricati all'avvio: {self.iframe_hosts}")
        logger.info(f"Auth URL: {self.auth_url}")
        logger.info(f"Stream CDN Template: {self.stream_cdn_template}")
        logger.info(f"Stream Other Template: {self.stream_other_template}")
        logger.info(f"Heartbeat URL: {self.heartbeat_url}")
        logger.info(f"Server Lookup URL: {self.server_lookup_url}")
        logger.info(f"Base Domain: {self.base_domain}")

    def _load_cache(self) -> Dict[str, Any]:
        """Carica la cache da un file codificato in Base64 all'avvio. Ritorna struttura completa."""
        try:
            if os.path.exists(self.cache_file):
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    logger.info(f"üíæ Caricamento cache dal file: {self.cache_file}")
                    encoded_data = f.read()
                    if not encoded_data:
                         # Tenta di gestire il vecchio formato o ritorna default
                        return {'hosts': [], 'streams': {}}
                    
                    try:
                        decoded_data = base64.b64decode(encoded_data).decode('utf-8')
                        data = json.loads(decoded_data)
                        
                        # Retrocompatibilit√†: se la cache √® il vecchio formato (dict di stream), convertilo
                        if data and not ('hosts' in data and 'streams' in data):
                             # Assumiamo che keys siano channel_ids
                             # Verifica euristica: se le chiavi sono stringhe numeriche, √® il vecchio formato
                             is_old_format = any(k.isdigit() for k in data.keys())
                             if is_old_format or not data:
                                 logger.info("Rilevato vecchio formato cache, conversione in corso...")
                                 return {'hosts': [], 'streams': data}
                        
                        return data
                    except Exception:
                        # Se fallisce decode, prova a leggerlo come JSON plain (fallback)
                        f.seek(0)
                        return json.load(f)

        except (IOError, json.JSONDecodeError, Exception) as e:
            logger.error(f"‚ùå Errore durante il caricamento della cache: {e}. Inizio con una cache vuota.")
        return {'hosts': [], 'streams': {}}

    def _get_random_proxy(self):
        """Restituisce un proxy casuale dalla lista."""
        return random.choice(self.proxies) if self.proxies else None

    async def _get_session(self):
        """‚úÖ Sessione persistente con cookie jar automatico"""
        if self.session is None or self.session.closed:
            timeout = ClientTimeout(total=60, connect=30, sock_read=30)
            proxy = self._get_random_proxy()
            if proxy:
                logger.info(f"üîó Utilizzo del proxy {proxy} per la sessione DLHD.")
                connector = ProxyConnector.from_url(proxy, ssl=False)
            else:
                connector = TCPConnector(
                    limit=0,
                    limit_per_host=0,
                    keepalive_timeout=30,
                    enable_cleanup_closed=True,
                    force_close=False,
                    use_dns_cache=True
                )
                logger.info("‚ÑπÔ∏è Nessun proxy specifico per DLHD, uso connessione diretta.")
            # ‚úÖ FONDAMENTALE: Cookie jar per mantenere sessione come browser reale
            self.session = ClientSession(
                timeout=timeout,
                connector=connector,
                headers=self.base_headers,
                cookie_jar=aiohttp.CookieJar()
            )
        return self.session

    def _save_cache(self):
        """Salva lo stato corrente della cache su un file, codificando il contenuto in Base64."""
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                # Struttura completa
                cache_data = {
                    'hosts': self.iframe_hosts,
                    'streams': self._stream_data_cache,
                    'auth_url': self.auth_url,
                    'stream_cdn_template': self.stream_cdn_template,
                    'stream_other_template': self.stream_other_template,
                    'heartbeat_url': self.heartbeat_url,
                    'server_lookup_url': self.server_lookup_url,
                    'base_domain': self.base_domain
                }
                json_data = json.dumps(cache_data)
                encoded_data = base64.b64encode(json_data.encode('utf-8')).decode('utf-8')
                f.write(encoded_data)
                logger.info(f"üíæ Cache (stream + hosts) salvata con successo.")
        except IOError as e:
            logger.error(f"‚ùå Errore durante il salvataggio della cache: {e}")

    async def _fetch_iframe_hosts(self) -> bool:
        """Scarica la lista aggiornata degli host iframe."""
        # URL offuscato per evitare scraping statico
        encoded_url = "aHR0cHM6Ly9pZnJhbWUuZGxoZC5kcGRucy5vcmcv"
        url = base64.b64decode(encoded_url).decode('utf-8')
        
        logger.info(f"üîÑ Aggiornamento lista host iframe...")
        try:
            session = await self._get_session()
            async with session.get(url, ssl=False, timeout=ClientTimeout(total=10)) as response:
                if response.status == 200:
                    text = await response.text()
                    # ‚úÖ Parsing con supporto per configurazione completa
                    lines = [line.strip() for line in text.splitlines() if line.strip()]
                    new_hosts = []
                    
                    for line in lines:
                        if line.startswith('#AUTH_URL:'):
                            self.auth_url = line.replace('#AUTH_URL:', '').strip()
                            logger.info(f"‚úÖ Auth URL aggiornato: {self.auth_url}")
                        elif line.startswith('#STREAM_CDN_TEMPLATE:'):
                            self.stream_cdn_template = line.replace('#STREAM_CDN_TEMPLATE:', '').strip()
                            logger.info(f"‚úÖ Stream CDN Template aggiornato: {self.stream_cdn_template}")
                        elif line.startswith('#STREAM_OTHER_TEMPLATE:'):
                            self.stream_other_template = line.replace('#STREAM_OTHER_TEMPLATE:', '').strip()
                            logger.info(f"‚úÖ Stream Other Template aggiornato: {self.stream_other_template}")
                        elif line.startswith('#HEARTBEAT_URL:'):
                            self.heartbeat_url = line.replace('#HEARTBEAT_URL:', '').strip()
                            logger.info(f"‚úÖ Heartbeat URL aggiornato: {self.heartbeat_url}")
                        elif line.startswith('#SERVER_LOOKUP_URL:'):
                            self.server_lookup_url = line.replace('#SERVER_LOOKUP_URL:', '').strip()
                            logger.info(f"‚úÖ Server Lookup URL aggiornato: {self.server_lookup_url}")
                        elif line.startswith('#BASE_DOMAIN:'):
                            self.base_domain = line.replace('#BASE_DOMAIN:', '').strip()
                            logger.info(f"‚úÖ Base Domain aggiornato: {self.base_domain}")
                        elif not line.startswith('#'):
                            new_hosts.append(line)
                    
                    if new_hosts:
                        self.iframe_hosts = new_hosts
                        logger.info(f"‚úÖ Lista host aggiornata: {self.iframe_hosts}")
                        self._save_cache()
                        return True
                    else:
                         logger.warning("‚ö†Ô∏è Lista host scaricata ma vuota.")
                else:
                    logger.error(f"‚ùå Errore HTTP {response.status} durante aggiornamento host iframe.")
        except Exception as e:
            logger.error(f"‚ùå Eccezione durante aggiornamento host iframe: {e}")
        
        return False

    def _get_headers_for_url(self, url: str, base_headers: dict) -> dict:
        """Applica headers specifici per il dominio stream automaticamente"""
        headers = base_headers.copy()
        parsed_url = urlparse(url)
        
        # Usa base_domain dinamico dal worker
        stream_domain = self.base_domain
        
        if stream_domain in parsed_url.netloc:
            origin = f"{parsed_url.scheme}://{parsed_url.netloc}"
            special_headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36',
                'Referer': origin,
                'Origin': origin
            }
            headers.update(special_headers)
        
        return headers

    async def _handle_response_content(self, response: aiohttp.ClientResponse) -> str:
        """Gestisce la decompressione manuale del corpo della risposta (zstd, gzip, etc.)."""
        content_encoding = response.headers.get('Content-Encoding')
        raw_body = await response.read()
        
        try:
            if content_encoding == 'zstd':
                logger.info(f"Rilevata compressione zstd per {response.url}. Decompressione in corso...")
                try:
                    dctx = zstandard.ZstdDecompressor()
                    # ‚úÖ MODIFICA: Utilizza stream_reader per gestire frame senza dimensione del contenuto.
                    # Questo risolve l'errore "could not determine content size in frame header".
                    with dctx.stream_reader(raw_body) as reader:
                        decompressed_body = reader.read()
                    return decompressed_body.decode(response.charset or 'utf-8')
                except zstandard.ZstdError as e:
                    logger.error(f"Errore di decompressione Zstd: {e}. Il contenuto potrebbe essere incompleto o corrotto.")
                    raise ExtractorError(f"Fallimento decompressione zstd: {e}")
            elif content_encoding == 'gzip':
                logger.info(f"Rilevata compressione gzip per {response.url}. Decompressione in corso...")
                decompressed_body = gzip.decompress(raw_body)
                return decompressed_body.decode(response.charset or 'utf-8')
            elif content_encoding == 'deflate':
                logger.info(f"Rilevata compressione deflate per {response.url}. Decompressione in corso...")
                decompressed_body = zlib.decompress(raw_body)
                return decompressed_body.decode(response.charset or 'utf-8')
            else:
                # Nessuna compressione o compressione non gestita, prova a decodificare direttamente
                # ‚úÖ FIX: Usa 'errors=replace' per evitare crash su byte non validi
                return raw_body.decode(response.charset or 'utf-8', errors='replace')
        except Exception as e:
            logger.error(f"Errore durante la decompressione/decodifica del contenuto da {response.url}: {e}")
            raise ExtractorError(f"Fallimento decompressione per {response.url}: {e}")

    async def _make_robust_request(self, url: str, headers: dict = None, retries=3, initial_delay=2):
        """‚úÖ Richieste con sessione persistente per evitare anti-bot"""
        final_headers = self._get_headers_for_url(url, headers or {})
        # Aggiungiamo zstd agli header accettati per segnalare al server che lo supportiamo
        # Rimosso 'br' perch√© non gestito in _handle_response_content
        final_headers['Accept-Encoding'] = 'gzip, deflate, zstd'
        
        for attempt in range(retries):
            try:
                # ‚úÖ IMPORTANTE: Riusa sempre la stessa sessione
                session = await self._get_session()
                
                logger.info(f"Tentativo {attempt + 1}/{retries} per URL: {url}")
                async with session.get(url, headers=final_headers, ssl=False, auto_decompress=False) as response:
                    response.raise_for_status()
                    content = await self._handle_response_content(response)
                    
                    class MockResponse:
                        def __init__(self, text_content, status, headers_dict):
                            self._text = text_content
                            self.status = status
                            self.headers = headers_dict
                            self.url = url
                        
                        async def text(self):
                            return self._text
                            
                        def raise_for_status(self):
                            if self.status >= 400:
                                raise aiohttp.ClientResponseError(
                                    request_info=None, 
                                    history=None,
                                    status=self.status
                                )
                        
                        async def json(self):
                            return json.loads(self._text)
                    
                    logger.info(f"‚úÖ Richiesta riuscita per {url} al tentativo {attempt + 1}")
                    return MockResponse(content, response.status, response.headers)
                    
            except (
                aiohttp.ClientConnectionError, 
                aiohttp.ServerDisconnectedError,
                aiohttp.ClientPayloadError,
                asyncio.TimeoutError,
                OSError,
                ConnectionResetError,
            ) as e:
                logger.warning(f"‚ö†Ô∏è Errore connessione tentativo {attempt + 1} per {url}: {str(e)}")
                
                # ‚úÖ Solo in caso di errore critico, chiudi la sessione
                if attempt == retries - 1:
                    if self.session and not self.session.closed:
                        try:
                            await self.session.close()
                        except:
                            pass
                    self.session = None
                
                if attempt < retries - 1:
                    delay = initial_delay * (2 ** attempt)
                    logger.info(f"‚è≥ Aspetto {delay} secondi prima del prossimo tentativo...")
                    await asyncio.sleep(delay)
                else:
                    raise ExtractorError(f"Tutti i {retries} tentativi falliti per {url}: {str(e)}")
                    
            except Exception as e:
                # Controlla se l'errore √® dovuto a zstd e logga un messaggio specifico
                if 'zstd' in str(e).lower():
                    logger.critical(f"‚ùå Errore critico con la decompressione zstd. Assicurati che la libreria 'zstandard' sia installata (`pip install zstandard`). Errore: {e}")
                else: # type: ignore
                    logger.error(f"‚ùå Errore non di rete tentativo {attempt + 1} per {url}: {str(e)}")
                if attempt == retries - 1:
                    raise ExtractorError(f"Errore finale per {url}: {str(e)}")
        await asyncio.sleep(initial_delay)

    async def extract(self, url: str, force_refresh: bool = False, **kwargs) -> Dict[str, Any]:
        """Flusso di estrazione principale: estrae direttamente dall'iframe."""
        
        async def get_stream_data_direct(channel_id: str, hosts_to_try: list) -> Dict[str, Any]:
            """Estrazione diretta dall'iframe senza passare per la pagina principale."""
            
            user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36'
            
            last_error = None
            for iframe_host in hosts_to_try:
                try:
                    iframe_url = f'https://{iframe_host}/premiumtv/daddyhd.php?id={channel_id}'
                    logger.info(f"üîç Tentativo estrazione da: {iframe_url}")
                    
                    embed_headers = {
                        'User-Agent': user_agent,
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                        'Accept-Language': 'en-US,en;q=0.9',
                        'Referer': 'https://dlhd.dad/',
                        'sec-ch-ua': '"Chromium";v="136", "Google Chrome";v="136"',
                        'sec-ch-ua-mobile': '?0',
                        'sec-ch-ua-platform': '"macOS"',
                    }
                    
                    # Step 1: Fetch iframe page
                    resp = await self._make_robust_request(iframe_url, headers=embed_headers, retries=2)
                    js_content = await resp.text()
                    
                    # Check if it's lovecdn
                    if 'lovecdn.ru' in js_content:
                        logger.info("Detected lovecdn.ru content - using alternative extraction")
                        result = await self._extract_lovecdn_stream(iframe_url, js_content, embed_headers)
                        return result
                    
                    # Step 2: Extract auth params
                    params = {}
                    patterns = {
                        'channel_key': r'(?:const|var|let)\s+(?:CHANNEL_KEY|channelKey)\s*=\s*["\']([^"\']+)["\']',
                        'auth_token': r'(?:const|var|let)\s+AUTH_TOKEN\s*=\s*["\']([^"\']+)["\']',
                        'auth_country': r'(?:const|var|let)\s+AUTH_COUNTRY\s*=\s*["\']([^"\']+)["\']',
                        'auth_ts': r'(?:const|var|let)\s+AUTH_TS\s*=\s*["\']([^"\']+)["\']',
                        'auth_expiry': r'(?:const|var|let)\s+AUTH_EXPIRY\s*=\s*["\']([^"\']+)["\']',
                    }
                    for key, pattern in patterns.items():
                        match = re.search(pattern, js_content)
                        params[key] = match.group(1) if match else None
                    
                    missing_params = [k for k, v in params.items() if not v]
                    if missing_params:
                        logger.warning(f"‚ö†Ô∏è Parametri mancanti da {iframe_host}: {missing_params}. Tentativo con nuovo flusso euristico...")
                        try:
                            # Se mancano i parametri standard, prova il nuovo flusso euristico/obfuscated
                            result = await self._extract_new_auth_flow(iframe_url, js_content, embed_headers)
                            return result
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è Nuovo flusso fallito: {e}")
                            last_error = ExtractorError(f"Missing params: {missing_params} and New Flow failed")
                            continue
                    
                    logger.info(f"‚úÖ Parametri estratti: channel_key={params['channel_key']}")
                    
                    # Step 3: Auth POST
                    # ‚úÖ DINAMICO: usa self.auth_url completo
                    auth_url = self.auth_url
                    logger.info(f"üîê Usando auth_url: {auth_url}")
                    iframe_origin = f"https://{iframe_host}"
                    
                    form_data = FormData()
                    form_data.add_field('channelKey', params['channel_key'])
                    form_data.add_field('country', params['auth_country'])
                    form_data.add_field('timestamp', params['auth_ts'])
                    form_data.add_field('expiry', params['auth_expiry'])
                    form_data.add_field('token', params['auth_token'])
                    
                    auth_headers = {
                        'User-Agent': user_agent,
                        'Accept': '*/*',
                        'Accept-Language': 'en-US,en;q=0.9',
                        'Content-Type': 'application/x-www-form-urlencoded',
                        'Origin': iframe_origin,
                        'Referer': iframe_url,
                        'Sec-Fetch-Dest': 'empty',
                        'Sec-Fetch-Mode': 'cors',
                        'Sec-Fetch-Site': 'cross-site',
                        'sec-ch-ua': '"Chromium";v="136", "Google Chrome";v="136"',
                        'sec-ch-ua-mobile': '?0',
                        'sec-ch-ua-platform': '"macOS"',
                    }
                    
                    session = await self._get_session()
                    async with session.post(auth_url, data=form_data, headers=auth_headers, ssl=False) as auth_resp:
                        auth_text = await auth_resp.text()
                        logger.info(f"Auth response: {auth_resp.status} - {auth_text[:100]}")
                        
                        if auth_resp.status != 200 or 'Blocked' in auth_text or 'bad params' in auth_text.lower():
                            logger.warning(f"‚ö†Ô∏è Auth bloccato da {iframe_host}: {auth_text[:50]}")
                            
                            
                            # ‚úÖ NUOVO: Se √® il primo host e auth fallisce, prova a refreshare config
                            if iframe_host == hosts_to_try[0] and not getattr(self, '_config_refreshed', False):
                                logger.info("üîÑ Auth fallito, provo ad aggiornare config dal worker...")
                                self._config_refreshed = True
                                if await self._fetch_iframe_hosts():
                                    # Aggiorna auth_url per il prossimo tentativo
                                    auth_url = self.auth_url
                                    logger.info(f"‚úÖ Config aggiornata, nuovo auth_url: {auth_url}")
                            
                            # ‚úÖ TENTATIVO NUOVO FLUSSO se Auth fallisce (es. token invalidi)
                            logger.warning("‚ö†Ô∏è Auth fallito con metodo standard. Tento nuovo flusso euristico...")
                            try:
                                result = await self._extract_new_auth_flow(iframe_url, js_content, embed_headers)
                                return result
                            except Exception as e:
                                logger.warning(f"‚ö†Ô∏è Nuovo flusso (fallback) fallito: {e}")
                                last_error = ExtractorError(f"Auth blocked: {auth_text} AND New Flow failed: {e}")
                                continue
                        
                        try:
                            auth_data = json.loads(auth_text)
                            if not (auth_data.get('success') or auth_data.get('valid')):
                                logger.warning(f"‚ö†Ô∏è Auth fallito: {auth_data}")
                                last_error = ExtractorError(f"Auth failed: {auth_data}")
                                continue
                        except json.JSONDecodeError:
                            last_error = ExtractorError(f"Auth response not JSON: {auth_text}")
                            continue
                    
                    logger.info("‚úÖ Auth riuscito!")
                    
                    # ‚úÖ DEBUG: Log cookies e headers dalla risposta auth
                    auth_cookies = auth_resp.cookies
                    logger.info(f"üç™ Cookies dalla risposta auth: {dict(auth_cookies)}")
                    logger.info(f"üìã Headers dalla risposta auth: {dict(auth_resp.headers)}")
                    
                    # Log tutti i cookies nella session dopo auth
                    all_session_cookies = list(session.cookie_jar)
                    logger.info(f"üç™ Tutti i cookies nella sessione dopo auth: {all_session_cookies}")
                    
                    # Step 4: Server Lookup
                    # ‚úÖ Usa server_lookup_url dinamico dal worker
                    server_lookup_url = f"{self.server_lookup_url}?channel_id={params['channel_key']}"
                    lookup_headers = {
                        'User-Agent': user_agent,
                        'Accept': '*/*',
                        'Referer': iframe_url,
                        'Origin': iframe_origin,
                    }
                    
                    lookup_resp = await self._make_robust_request(server_lookup_url, headers=lookup_headers, retries=2)
                    server_data = await lookup_resp.json()
                    server_key = server_data.get('server_key')
                    
                    if not server_key:
                        last_error = ExtractorError(f"No server_key in response: {server_data}")
                        continue
                    
                    logger.info(f"‚úÖ Server key: {server_key}")
                    
                    # Step 5: Heartbeat - NECESSARIO per stabilire la sessione prima di ricevere le chiavi
                    channel_key = params['channel_key']
                    auth_token = params['auth_token']
                    
                    # ‚úÖ Usa heartbeat_url dinamico dal worker
                    heartbeat_url = self.heartbeat_url
                    heartbeat_headers = {
                        'User-Agent': user_agent,
                        'Authorization': f'Bearer {auth_token}',
                        'X-Channel-Key': channel_key,
                        'Referer': iframe_url,
                        'Origin': iframe_origin,
                    }
                    
                    try:
                        logger.info(f"üíì Invio heartbeat a: {heartbeat_url}")
                        async with session.get(heartbeat_url, headers=heartbeat_headers, ssl=False, timeout=ClientTimeout(total=10)) as hb_resp:
                            hb_text = await hb_resp.text()
                            logger.info(f"üíì Heartbeat response: {hb_resp.status} - {hb_text[:100]}")
                            if hb_resp.status != 200:
                                logger.warning(f"‚ö†Ô∏è Heartbeat non-200: {hb_resp.status}")
                    except Exception as hb_e:
                        logger.warning(f"‚ö†Ô∏è Heartbeat fallito: {hb_e}")
                        # Non blocchiamo l'estrazione se il heartbeat fallisce
                    
                    # Step 6: Build final URL
                    channel_key = params['channel_key']
                    auth_token = params['auth_token']
                    
                    # ‚úÖ DINAMICO: usa templates completi
                    if server_key == 'top1/cdn':
                        # Usa stream_cdn_template (es: https://top1.giokko.ru/top1/cdn/{CHANNEL}/mono.css)
                        stream_url = self.stream_cdn_template.replace('{CHANNEL}', channel_key)
                    else:
                        # Usa stream_other_template (es: https://{SERVER_KEY}new.giokko.ru/{SERVER_KEY}/{CHANNEL}/mono.css)
                        stream_url = self.stream_other_template.replace('{SERVER_KEY}', server_key).replace('{CHANNEL}', channel_key)
                    
                    logger.info(f"‚úÖ Stream URL costruito: {stream_url}")
                    
                    # ‚úÖ Genera X-Client-Token (richiesto dal provider per heartbeat/chiavi)
                    # Formula: btoa(CHANNEL_KEY|AUTH_COUNTRY|AUTH_TS|UA|fingerprint)
                    # fingerprint = UA|screen|timezone|lang
                    auth_ts = params.get('auth_ts', '')
                    auth_country = params.get('auth_country', 'IT')
                    screen_res = "1920x1080"  # Simula risoluzione comune
                    timezone = "Europe/Rome"
                    lang = "it-IT"
                    fingerprint = f"{user_agent}|{screen_res}|{timezone}|{lang}"
                    sign_data = f"{channel_key}|{auth_country}|{auth_ts}|{user_agent}|{fingerprint}"
                    client_token = base64.b64encode(sign_data.encode('utf-8')).decode('utf-8')
                    logger.info(f"üîê X-Client-Token generato per channel {channel_key}")
                    
                    stream_headers = {
                        'User-Agent': user_agent,
                        'Referer': iframe_url,
                        'Origin': iframe_origin,
                        'Authorization': f'Bearer {auth_token}',
                        'X-Channel-Key': channel_key,
                        'Heartbeat-Url': self.heartbeat_url,  # ‚úÖ Passato al proxy per le richieste chiave
                        'X-Client-Token': client_token,  # ‚úÖ Token richiesto per heartbeat/chiavi
                    }

                    # ‚úÖ Aggiungi cookies dalla sessione corrente
                    if self.session:
                        # Log all cookies for debugging
                        all_cookies = list(self.session.cookie_jar)
                        logger.info(f"üç™ All cookies in jar: {all_cookies}")
                        
                        cookies = self.session.cookie_jar.filter_cookies(stream_url)
                        cookie_str = "; ".join([f"{k}={v.value}" for k, v in cookies.items()])
                        if cookie_str:
                            stream_headers['Cookie'] = cookie_str
                            logger.info(f"üç™ Cookies aggiunti agli headers: {cookie_str[:50]}...")

                    expires_at = None
                    try:
                        if params.get('auth_expiry'):
                            expires_at = float(params['auth_expiry'])
                            logger.info(f"‚è≥ Auth Expiry: {expires_at} (Current time: {time.time()})")
                    except (ValueError, TypeError):
                        pass
                    
                    # ‚úÖ Reset flag per permettere futuri refresh
                    self._config_refreshed = False
                    
                    return {
                        "destination_url": stream_url,
                        "request_headers": stream_headers,
                        "mediaflow_endpoint": self.mediaflow_endpoint,
                        "expires_at": expires_at
                    }
                    
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Errore con {iframe_host}: {e}")
                    last_error = e
                    continue
            
            raise ExtractorError(f"Tutti gli host iframe hanno fallito. Ultimo errore: {last_error}")

        # Helper per estrarre ID (spostato fuori per pulizia scope)
        def extract_channel_id(u: str) -> Optional[str]:
            patterns = [
                r'/premium(\d+)/mono',
                r'/(?:watch|stream|cast|player)/stream-(\d+)\.php',
                r'watch\.php\?id=(\d+)',
                r'(?:%2F|/)stream-(\d+)\.php',
                r'stream-(\d+)\.php',
                r'[?&]id=(\d+)',
                r'daddyhd\.php\?id=(\d+)',
            ]
            for pattern in patterns:
                match = re.search(pattern, u, re.IGNORECASE)
                if match:
                    return match.group(1)
            return None

        try:
            channel_id = extract_channel_id(url)
            if not channel_id:
                raise ExtractorError(f"Impossibile estrarre channel ID da {url}")

            logger.info(f"üì∫ Estrazione per canale ID: {channel_id}")

            # Controlla la cache prima di procedere
            if not force_refresh and channel_id in self._stream_data_cache:
                logger.info(f"‚úÖ Trovati dati in cache per il canale ID: {channel_id}. Verifico la validit√†...")
                cached_data = self._stream_data_cache[channel_id]
                stream_url = cached_data.get("destination_url")
                stream_headers = cached_data.get("request_headers", {})
                expires_at = cached_data.get("expires_at")

                is_valid = False
                
                # ‚úÖ Check expiry first (con buffer di 30 secondi)
                if expires_at and time.time() > (expires_at - 30):
                     logger.warning(f"‚ö†Ô∏è Cache scaduta per il canale ID {channel_id} (expires_at: {expires_at}).")
                     is_valid = False
                elif stream_url:
                    try:
                        async with aiohttp.ClientSession(timeout=ClientTimeout(total=10)) as validation_session:
                            async with validation_session.head(stream_url, headers=stream_headers, ssl=False) as response:
                                if response.status == 200:
                                    is_valid = True
                                    logger.info(f"‚úÖ Cache per il canale ID {channel_id} √® valida.")
                                else:
                                    logger.warning(f"‚ö†Ô∏è Cache per il canale ID {channel_id} non valida. Status: {response.status}. Procedo con estrazione.")
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Errore durante la validazione della cache per {channel_id}: {e}. Procedo con estrazione.")
                
                if not is_valid:
                    if channel_id in self._stream_data_cache:
                        del self._stream_data_cache[channel_id]
                    self._save_cache()
                    logger.info(f"üóëÔ∏è Cache invalidata per il canale ID {channel_id}.")
                else:
                    return cached_data

            # Usa un lock per prevenire estrazioni simultanee per lo stesso canale
            if channel_id not in self._extraction_locks:
                self._extraction_locks[channel_id] = asyncio.Lock()
            
            lock = self._extraction_locks[channel_id]
            async with lock:
                # Ricontrolla la cache dopo aver acquisito il lock
                if not force_refresh and channel_id in self._stream_data_cache:
                    cached_data = self._stream_data_cache[channel_id]
                    expires_at = cached_data.get("expires_at")
                    
                    # Se √® scaduta anche la nuova cache (improbabile ma possibile), procedi con estrazione
                    if expires_at and time.time() > (expires_at - 30):
                        logger.info(f"‚ö†Ô∏è Cache (ricontrollata) scaduta per {channel_id}, procedo con nuova estrazione.")
                    else:
                        logger.info(f"‚úÖ Dati per il canale {channel_id} trovati in cache dopo aver atteso il lock.")
                        return self._stream_data_cache[channel_id]

                # Procedi con l'estrazione diretta
                # Procedi con l'estrazione diretta
                logger.info(f"‚öôÔ∏è Nessuna cache valida per {channel_id}, avvio estrazione diretta...")
                
                try:
                    result = await get_stream_data_direct(channel_id, self.iframe_hosts)
                except ExtractorError:
                    # Se fallisce con gli host correnti, prova ad aggiornarli
                    logger.warning("‚ö†Ô∏è Tutti gli host correnti hanno fallito. Tento aggiornamento lista host...")
                    if await self._fetch_iframe_hosts():
                         logger.info(f"üîÑ Riprovo con nuovi host: {self.iframe_hosts}")
                         result = await get_stream_data_direct(channel_id, self.iframe_hosts)
                    else:
                        raise
                
                # Salva in cache
                self._stream_data_cache[channel_id] = result
                self._save_cache()
                
                return result
            
        except Exception as e:
            # Per errori 403, non loggare il traceback perch√© sono errori attesi (servizio temporaneamente non disponibile)
            error_message = str(e)
            if "403" in error_message or "Forbidden" in error_message:
                logger.error(f"Estrazione DLHD completamente fallita per URL {url}")
            else:
                logger.exception(f"Estrazione DLHD completamente fallita per URL {url}")
            raise ExtractorError(f"Estrazione DLHD completamente fallita: {str(e)}")

    async def _extract_lovecdn_stream(self, iframe_url: str, iframe_content: str, headers: dict) -> Dict[str, Any]:
        """
        Estrattore alternativo per iframe lovecdn.ru che usa un formato diverso.
        """
        try:
            # Cerca pattern di stream URL diretto
            m3u8_patterns = [
                r'["\']([^"\']*\.m3u8[^"\']*)["\']',
                r'source[:\s]+["\']([^"\']+)["\']',
                r'file[:\s]+["\']([^"\']+\.m3u8[^"\']*)["\']',
                r'hlsManifestUrl[:\s]*["\']([^"\']+)["\']',
            ]
            
            stream_url = None
            for pattern in m3u8_patterns:
                matches = re.findall(pattern, iframe_content)
                for match in matches:
                    if '.m3u8' in match and match.startswith('http'):
                        stream_url = match
                        logger.info(f"Found direct m3u8 URL: {stream_url}")
                        break
                if stream_url:
                    break
            
            # Pattern 2: Cerca costruzione dinamica URL
            if not stream_url:
                channel_match = re.search(r'(?:stream|channel)["\s:=]+["\']([^"\']+)["\']', iframe_content)
                server_match = re.search(r'(?:server|domain|host)["\s:=]+["\']([^"\']+)["\']', iframe_content)
                
                if channel_match:
                    channel_name = channel_match.group(1)
                    # Usa base_domain dinamico
                    server = server_match.group(1) if server_match else self.base_domain
                    stream_url = f"https://{server}/{channel_name}/mono.m3u8"
                    logger.info(f"Constructed stream URL: {stream_url}")
            
            if not stream_url:
                # Fallback: cerca qualsiasi URL che sembri uno stream
                url_pattern = r'https?://[^\s"\'<>]+\.m3u8[^\s"\'<>]*'
                matches = re.findall(url_pattern, iframe_content)
                if matches:
                    stream_url = matches[0]
                    logger.info(f"Found fallback stream URL: {stream_url}")
            
            if not stream_url:
                raise ExtractorError(f"Could not find stream URL in lovecdn.ru iframe")
            
            # Usa iframe URL come referer
            iframe_origin = f"https://{urlparse(iframe_url).netloc}"
            stream_headers = {
                'User-Agent': headers['User-Agent'],
                'Referer': iframe_url,
                'Origin': iframe_origin
            }
            
            # Determina endpoint in base al dominio dello stream
            # Se √® un m3u8 standard, usa hls_manifest_proxy, se richiede chiavi speciali, potrebbe servire altro
            # Ma per ora manteniamo hls_manifest_proxy come nel codice originale
            
            logger.info(f"Using lovecdn.ru stream extraction")
            
            return {
                "destination_url": stream_url,
                "request_headers": stream_headers,
                "mediaflow_endpoint": self.mediaflow_endpoint,
            }
            
        except Exception as e:
            raise ExtractorError(f"Failed to extract lovecdn.ru stream: {e}")

    async def _extract_new_auth_flow(self, iframe_url: str, iframe_content: str, headers: dict) -> Dict[str, Any]:
        """Gestisce il nuovo flusso di autenticazione con estrazione euristica."""
        
        logger.info("Tentativo rilevamento nuovo flusso auth obfuscated...")
        
        # 1. Estrazione euristica delle variabili
        params = {}
        
        # Cerca il JWT (inizia con eyJ...)
        jwt_match = re.search(r'["\'](eyJ[a-zA-Z0-9\-_]+\.[a-zA-Z0-9\-_]+\.[a-zA-Z0-9\-_]+)["\']', iframe_content)
        if jwt_match:
            params['auth_token'] = jwt_match.group(1)
            logger.info("‚úÖ Trovato possibile JWT Token")
            
        # Cerca Channel Key (es: premium853) - o stringa alfanumerica di media lunghezza
        # Spesso assegnata a una variabile corta
        key_matches = re.finditer(r'["\']([a-z]+[0-9]+)["\']', iframe_content)
        for m in key_matches:
            val = m.group(1)
            # Filtro euristico: deve sembrare una chiave canale (es. dad123, premium853, etc)
            if re.match(r'^(premium|dad|sport|live)[0-9]+$', val) or '853' in val: # Harcoded check per debug
                params['channel_key'] = val
                logger.info(f"‚úÖ Trovata possibile Channel Key: {val}")
                break
                
        # Cerca Country (2 lettere maiuscole)
        country_match = re.search(r'["\']([A-Z]{2})["\']', iframe_content)
        if country_match:
            params['auth_country'] = country_match.group(1)
        else:
             params['auth_country'] = 'DE' # Fallback
             
        # Cerca Timestamp (10 cifre)
        ts_matches = re.findall(r'["\']([0-9]{10})["\']', iframe_content)
        if ts_matches:
            # Assumiamo che il primo sia iat/ts e il secondo exp, o viceversa.
            # Prendi il pi√π piccolo come TS corrente
            ts_values = sorted([int(x) for x in ts_matches])
            params['auth_ts'] = str(ts_values[0])
            if len(ts_values) > 1:
                params['auth_expiry'] = str(ts_values[-1])
            else:
                params['auth_expiry'] = str(ts_values[0] + 3600)
        
        # Validazione minima
        if not params.get('auth_token'):
             raise ExtractorError("Impossibile estrarre JWT dal nuovo flusso.")
             
        # Se manca channel key, prova a estrarla dall'URL
        if not params.get('channel_key'):
             # fallback da URL iframe o parametro passato
             pass # Gestito dal chiamante se fallisce qui
        
        logger.info(f"‚úÖ Parametri euristici estratti: {params}")

        # 2. SKIP auth2.php POST - Il nuovo flusso usa direttamente il token nel heartbeat
        # L'errore "INVALID_TOKEN" su auth2.php suggerisce che quel passaggio √® deprecato o rotto per questo flusso.
        
        logger.info("üöÄ Skipping auth2.php POST (nuovo flusso detectato). Procedo diretto al heartbeat.")
        
        # 3. Server Lookup & Heartbeat Setup
        auth_token = params['auth_token']
        # Se channel_key non trovato nel JS, prova a derivarlo dall'URL iframe originale se possibile,
        # ma qui assumiamo che il chiamante (extract) l'abbia passato o che lo troviamo.
        # Per ora usiamo quello trovato o falliamo.
        
        channel_key = params.get('channel_key')
        if not channel_key:
             # Tentativo estremo: estrai da URL iframe
             m_url = re.search(r'id=([0-9]+)', iframe_url)
             if m_url:
                 # Spesso la key √® 'premium' + id
                 channel_key = f"premium{m_url.group(1)}"
                 logger.info(f"‚ö†Ô∏è Channel Key non trovata nel JS, indovinata dall'URL: {channel_key}")
             else:
                 raise ExtractorError("Channel Key mancante e non derivabile.")

        # 4. Server Lookup
        user_agent = headers.get('User-Agent')
        iframe_origin = f"https://{urlparse(iframe_url).netloc}"
        
        server_lookup_url = f"{self.server_lookup_url}?channel_id={channel_key}"
        lookup_headers = {
            'User-Agent': user_agent,
            'Accept': '*/*',
            'Referer': iframe_url,
            'Origin': iframe_origin,
        }
        
        logger.info(f"üîç Server Lookup su: {server_lookup_url}")
        lookup_resp = await self._make_robust_request(server_lookup_url, headers=lookup_headers, retries=2)
        server_data = await lookup_resp.json()
        server_key = server_data.get('server_key')
        
        if not server_key:
            raise ExtractorError(f"No server_key in response: {server_data}")
        
        logger.info(f"‚úÖ Server key: {server_key}")
        
        # 5. Heartbeat
        heartbeat_url = self.heartbeat_url
        
        # Genera X-Client-Token (stessa logica)
        auth_country = params.get('auth_country', 'DE')
        auth_ts = params.get('auth_ts', str(int(time.time())))
        screen_res = "1920x1080"
        timezone = "Europe/Berlin" 
        lang = "en-US"
        fingerprint = f"{user_agent}|{screen_res}|{timezone}|{lang}"
        sign_data = f"{channel_key}|{auth_country}|{auth_ts}|{user_agent}|{fingerprint}"
        client_token = base64.b64encode(sign_data.encode('utf-8')).decode('utf-8')
        
        heartbeat_headers = {
            'User-Agent': user_agent,
            'Authorization': f'Bearer {auth_token}',
            'X-Channel-Key': channel_key,
            'X-Client-Token': client_token,
            'Referer': iframe_url,
            'Origin': iframe_origin,
        }
        
        try:
            logger.info(f"üíì Invio heartbeat (diretto) a: {heartbeat_url}")
            async with self.session.get(heartbeat_url, headers=heartbeat_headers, ssl=False, timeout=ClientTimeout(total=10)) as hb_resp:
                hb_text = await hb_resp.text()
                logger.info(f"üíì Heartbeat response: {hb_resp.status} - {hb_text[:100]}")
        except Exception as hb_e:
            logger.warning(f"‚ö†Ô∏è Heartbeat fallito: {hb_e}")
            
        # 6. Build Stream URL
        if server_key == 'top1/cdn':
            stream_url = self.stream_cdn_template.replace('{CHANNEL}', channel_key)
        else:
            stream_url = self.stream_other_template.replace('{SERVER_KEY}', server_key).replace('{CHANNEL}', channel_key)
            
        logger.info(f"‚úÖ Stream URL costruito: {stream_url}")
        
        stream_headers = {
            'User-Agent': user_agent,
            'Referer': iframe_url,
            'Origin': iframe_origin,
            'Authorization': f'Bearer {auth_token}',
            'X-Channel-Key': channel_key,
            'Heartbeat-Url': self.heartbeat_url,
            'X-Client-Token': client_token,
        }
        
        return {
            "destination_url": stream_url,
            "request_headers": stream_headers,
            "mediaflow_endpoint": self.mediaflow_endpoint,
            "expires_at": float(params.get('auth_expiry', 0))
        }

        try:
            session = await self._get_session()
            async with session.post(auth_url, data=form_data, headers=auth_headers, ssl=False) as auth_resp:
                auth_resp.raise_for_status()
                auth_data = await auth_resp.json()
                if not (auth_data.get("valid") or auth_data.get("success")):
                    raise ExtractorError(f"Initial auth failed with response: {auth_data}")
            logger.info("New auth flow: Initial auth successful.")
        except Exception as e:
            raise ExtractorError(f"New auth flow failed during initial auth POST: {e}")

        # 2. Server Lookup
        # ‚úÖ Usa server_lookup_url dinamico dal worker
        server_lookup_url = f"{self.server_lookup_url}?channel_id={params['channel_key']}"
        try:
            lookup_resp = await self._make_robust_request(server_lookup_url, headers=headers)
            server_data = await lookup_resp.json()
            server_key = server_data.get('server_key')
            if not server_key:
                raise ExtractorError(f"No server_key in lookup response: {server_data}")
            logger.info(f"New auth flow: Server lookup successful - Server key: {server_key}")
        except Exception as e:
            raise ExtractorError(f"New auth flow failed during server lookup: {e}")

        # 3. Build final stream URL
        channel_key = params['channel_key']
        auth_token = params['auth_token']
        # The JS logic uses .css, not .m3u8
        if server_key == 'top1/cdn':
            # Usa stream_cdn_template (es: https://top1.giokko.ru/top1/cdn/{CHANNEL}/mono.css)
            stream_url = self.stream_cdn_template.replace('{CHANNEL}', channel_key)
        else:
            # Usa stream_other_template (es: https://{SERVER_KEY}new.giokko.ru/{SERVER_KEY}/{CHANNEL}/mono.css)
            stream_url = self.stream_other_template.replace('{SERVER_KEY}', server_key).replace('{CHANNEL}', channel_key)
        
        logger.info(f'New auth flow: Constructed stream URL: {stream_url}')

        stream_headers = {
            'User-Agent': headers['User-Agent'],
            'Referer': iframe_url,
            'Origin': iframe_origin,
            'Authorization': f'Bearer {auth_token}',
            'X-Channel-Key': channel_key
        }

        return {
            "destination_url": stream_url,
            "request_headers": stream_headers,
            "mediaflow_endpoint": self.mediaflow_endpoint,
        }

    async def invalidate_cache_for_url(self, url: str):
        """
        Invalida la cache per un URL specifico.
        Questa funzione viene chiamata da app.py quando rileva un errore (es. fallimento chiave AES).
        """
        def extract_channel_id_internal(u: str) -> Optional[str]:
            patterns = [
                r'/premium(\d+)/mono\.m3u8$',
                r'/(?:watch|stream|cast|player)/stream-(\d+)\.php',
                r'watch\.php\?id=(\d+)',
                r'(?:%2F|/)stream-(\d+)\.php',
                r'stream-(\d+)\.php'
            ]
            for pattern in patterns:
                match = re.search(pattern, u, re.IGNORECASE)
                if match: return match.group(1)
            return None

        channel_id = extract_channel_id_internal(url)
        if channel_id and channel_id in self._stream_data_cache:
            del self._stream_data_cache[channel_id]
            self._save_cache()
            logger.info(f"üóëÔ∏è Cache per il canale ID {channel_id} invalidata a causa di un errore esterno (es. chiave AES).")

    async def close(self):
        """Chiude definitivamente la sessione"""
        if self.session and not self.session.closed:
            try:
                await self.session.close()
            except:
                pass
        self.session = None
